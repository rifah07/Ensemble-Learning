{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (eXtreme Gradient Boosting) is one of the most popular and powerful boosting techniques in machine learning, particularly for **structured/tabular data**.\n",
        "It is an advanced implementation of Gradient Boosting, optimized for speed and performance."
      ],
      "metadata": {
        "id": "40CdIdoS54PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is a scalable and efficient version of Gradient Boosting. It incorporates several enhancements over traditional Gradient Boosting, including:\n",
        "\n",
        "1. Regularization to prevent overfitting.\n",
        "2. Parallelization to speed up training.\n",
        "3. Handling missing values natively.\n",
        "4. Tree pruning for better optimization.\n",
        "5. Custom loss functions for flexibility."
      ],
      "metadata": {
        "id": "ud38VbHb6DWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features of XGBoost\n",
        "\n",
        "1. Regularization:\n",
        "     * Adds L1 (Lasso) and L2 (Ridge) regularization terms to the loss function to control model complexity.\n",
        "\n",
        "2. Tree Pruning:\n",
        "    * Uses \"max_depth\" instead of the \"depth-first\" splitting to stop trees early, avoiding unnecessary splits.\n",
        "3. Weighted Quantile Sketch:\n",
        "    * Handles weighted datasets efficiently for split-point selection.\n",
        "4. Parallel Processing:\n",
        "    * Boosting itself is sequential, but operations like finding the best split for trees are parallelized.\n",
        "5. Sparsity Awareness:\n",
        "      * Handles missing values gracefully by learning the best way to deal with them."
      ],
      "metadata": {
        "id": "wwVCOcpV6R3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s an example of using XGBoost for classification on the Iris dataset."
      ],
      "metadata": {
        "id": "B4EakCMM6pvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIoRM2P55nBj",
        "outputId": "54f87d80-d088-4e79-8181-4bc846ace1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "XDV0rHLR6t-O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "sbr321n_6zUn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "6v5CQDpY61-I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DMatrix (optimized data structure for XGBoost)\n",
        "\n",
        "train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
        "test_data = xgb.DMatrix(data=X_test, label=y_test)"
      ],
      "metadata": {
        "id": "2PGwGWQK65Ey"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the XGBoost model\n",
        "params = {\n",
        "    \"objective\": \"multi:softmax\",  # For classification with multiple classes\n",
        "    \"num_class\": 3,                # Number of classes in the target\n",
        "    \"max_depth\": 3,                # Maximum depth of trees\n",
        "    \"eta\": 0.1,                    # Learning rate\n",
        "    \"subsample\": 0.8,              # Fraction of samples to use for training each tree\n",
        "    \"colsample_bytree\": 0.8,       # Fraction of features to use for each tree\n",
        "    \"seed\": 42                     # Random seed for reproducibility\n",
        "}"
      ],
      "metadata": {
        "id": "Wmgpo-MP67xQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the XGBoost model\n",
        "xgb_model = xgb.train(params, train_data, num_boost_round=100)"
      ],
      "metadata": {
        "id": "E1ApUFXZ7AIw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = xgb_model.predict(test_data)"
      ],
      "metadata": {
        "id": "LFGcmXTt6-fZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of XGBoost Classifier: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPIJW_iu7D4s",
        "outputId": "559f089e-2e7d-4625-d0c0-acc7465cdc0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of XGBoost Classifier: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Walkthrough\n",
        "\n",
        "1. Data Preparation:\n",
        "    * DMatrix: A data structure optimized for XGBoost, holding both features and labels.\n",
        "2. Model Parameters:\n",
        "    * objective=\"multi:softmax\": Specifies a classification task with multiple classes.\n",
        "    * num_class=3: Number of unique target classes in the dataset.\n",
        "    * max_depth=3: Restricts the depth of each tree to control overfitting.\n",
        "    * eta=0.1: Learning rate (controls how much each tree contributes to the model).\n",
        "    * subsample=0.8: Uses 80% of the training data for each tree.\n",
        "    * colsample_bytree=0.8: Uses 80% of the features for each tree.\n",
        "\n",
        "3. Training:\n",
        "    * xgb.train(): Trains the XGBoost model sequentially on residual errors.\n",
        "\n",
        "4. Prediction:\n",
        "    * Predictions are made using the trained model on the test data.\n",
        "\n",
        "5. Evaluation:\n",
        "    * Accuracy is calculated to evaluate the model's performance."
      ],
      "metadata": {
        "id": "ihKKvPKh7bRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of XGBoost\n",
        "\n",
        "1. Extremely fast and efficient.\n",
        "2. Highly flexible (supports various objectives and custom loss functions).\n",
        "3. Handles missing data automatically.\n",
        "4. Outperforms traditional Gradient Boosting in many scenarios."
      ],
      "metadata": {
        "id": "ktsgY49j75OX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of XGBoost\n",
        "* May require careful hyperparameter tuning for best performance.\n",
        "* Computationally intensive for very large datasets."
      ],
      "metadata": {
        "id": "slGbJucv8ALY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points:\n",
        "\n",
        "1. Binary Classification: If the task were binary classification (e.g., 0 and 1), we would use \"objective\": \"binary:logistic\", and num_class wouldn’t be needed.\n",
        "\n",
        "2. Multi-Class Classification: For multi-class tasks like the Iris dataset, \"multi:softmax\" ensures the model handles all three classes correctly."
      ],
      "metadata": {
        "id": "flrDprF08vxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Each\n",
        "\n",
        "1. multi:softmax:\n",
        "    * Use when you only need the predicted class (e.g., 0, 1, or 2).\n",
        "\n",
        "2. multi:softprob:\n",
        "    * Use when you need the probabilities for all classes (e.g., [0.2, 0.5, 0.3])."
      ],
      "metadata": {
        "id": "ynnUIHlY86Zy"
      }
    }
  ]
}